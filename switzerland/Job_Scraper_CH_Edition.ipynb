{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Job Scraper Upgraded - CH Edition",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/job-scraper/blob/master/switzerland/Job_Scraper_CH_Edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PjVjR8lmmQx"
      },
      "source": [
        "# Custom Job Scraping + Viz\n",
        "\n",
        "---\n",
        "Created by: [Peter Szemraj](https://peterszemraj.ch/)\n",
        "\n",
        "1. [link](https://github.com/pszemraj/job-scraper) to the repo of this project\n",
        "\n",
        "---\n",
        "original code forked from [this article](https://towardsdatascience.com/automating-my-job-search-with-python-ee2b465c6a8f) on medium\n",
        "\n",
        "Key Features:\n",
        "1. connects to your google drive and saves the search files there\n",
        "2. clusters and visualizes job postings for faster \"find what I actually care about\"\n",
        "\n",
        "\n",
        "---\n",
        "**Note**\n",
        "\n",
        "If you are editing this code / adapting to a different country/site for Indeed, I commented <font color='orange'>**in orange font**</font> sections that I think likely need to be changed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hhorcuihr-u"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEigMy50jJ4h"
      },
      "source": [
        "### Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIyU8N7fjKzV"
      },
      "source": [
        "# create interface to upload / interact with google drive and video files\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# after you allow the authentication, you can work using the path \"/content/drive/My Drive\"\n",
        "# if it works it will say \"Mounted at /content/drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZvUCO1jj4Y"
      },
      "source": [
        "### Filepaths on Drive\n",
        "\n",
        "saves all runs to a folder in your google drive with today's date. Folder is created if it doesn't exist \n",
        "\n",
        "<font color='orange'> update the **directory** variable to a custom filepath if desired  </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXTiNdwKjdK5"
      },
      "source": [
        "from datetime import date\n",
        "import os\n",
        "from os.path import join, isdir\n",
        "today = date.today()\n",
        "# Month abbreviation, day and year\t\n",
        "td_date = today.strftime(\"%b-%d-%Y\")\n",
        "\n",
        "folder_name = \"job_search_results_\" + td_date\n",
        "directory = \"/content/drive/MyDrive/ethz_s2021/career/job_repo\"\n",
        "\n",
        "# check if transcription folder exists. If not, create it\n",
        "output_folder_path = join(directory, folder_name)\n",
        "\n",
        "if not isdir(output_folder_path):\n",
        "    os.mkdir(output_folder_path)  # make a place to store outputs if one does not exist\n",
        "\n",
        "    print(\"needed to create the folder: {}. Its location is: \\n\".format(folder_name),\n",
        "          output_folder_path)\n",
        "\n",
        "\n",
        "os.chdir(output_folder_path)\n",
        "print(\"\\n\\n the current working directory is: \\n\", output_folder_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDAk9c3TjKM0"
      },
      "source": [
        "### Install & Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tffFYWQPi-KF"
      },
      "source": [
        "%%capture\n",
        "\"\"\"\n",
        "libraries added / installed by Peter\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "!pip install -U selenium\n",
        "!pip install -U bs4\n",
        "!pip install -U texthero\n",
        "!pip install -U plotly\n",
        "!pip install pyshorteners\n",
        "!pip install sister\n",
        "!pip install -U kneed\n",
        "\n",
        "from os.path import join\n",
        "from google.colab import data_table\n",
        "import texthero as hero\n",
        "import pprint as pp\n",
        "import sister\n",
        "\n",
        "%load_ext google.colab.data_table\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Xu5tlXhpC8"
      },
      "source": [
        "\"\"\"\n",
        "libraries imported in the originally forked version of this code. \n",
        "\n",
        "Created on Tue Apr 28 11:35:04 2020\n",
        "@author: chrislovejoy\n",
        "\"\"\"\n",
        "import urllib\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2amAUOM5piqT"
      },
      "source": [
        "formats jupyter notebook / colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFxtFF3Ql_4W"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3JHDcx3hrsR"
      },
      "source": [
        "# Job Search Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOvrznfs46rr"
      },
      "source": [
        "## generic\n",
        "\n",
        "cuttly [website](https://cutt.ly/edit)\n",
        "cuttly tutorial w/ Python [here](https://towardsdatascience.com/best-apis-for-url-shortening-using-python-2db09d1f86f0)\n",
        "\n",
        "\n",
        "pyshorteners [docs](https://pyshorteners.readthedocs.io/en/latest/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zpez9VIrXaL"
      },
      "source": [
        "### misc.\n",
        "\n",
        "<font color='orange'> add own API key for link shortening as needed</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkf4LMbS2Ex6"
      },
      "source": [
        "import requests\n",
        "import time, random\n",
        "import pyshorteners\n",
        "\n",
        "def save_jobs_to_excel(jobs_list, filename, verbose=False):\n",
        "    # i have no idea what this function does\n",
        "    jobs = pd.DataFrame(jobs_list)\n",
        "    jobs.to_excel(filename)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"saved the following to excel with filename {}: \\n\".format(filename))\n",
        "\n",
        "        print(jobs.info())\n",
        "    return jobs\n",
        "\n",
        "def shorten_URL_bitly(long_url, verbose=False):\n",
        "    # requires free account / API token. https://bitly.com/\n",
        "    # generate short URLs from the ones scraped \n",
        "\n",
        "    time.sleep(random.randint(1,5)) # don't overload API\n",
        "\n",
        "    ACCESS_TOKEN = \"hahah_get_ur_own\"\n",
        "\n",
        "    # Shorten long URL\n",
        "    try:\n",
        "        s = pyshorteners.Shortener(api_key=ACCESS_TOKEN)\n",
        "        short_url = s.bitly.short(long_url)\n",
        "\n",
        "        if verbose:\n",
        "            print (\"Short URL is {}\".format(short_url))\n",
        "    except:\n",
        "        print(\"Error accessing API for key {} and fn shorten_URL_bitly\".format(ACCESS_TOKEN))\n",
        "        print(\"Try updating API key / checking fn. Returning original url\")\n",
        "        short_url = long_url\n",
        "        \n",
        "    return short_url\n",
        "\n",
        "\n",
        "def text_first_N(text, num=40):\n",
        "    # returns the first N chars in text, i.e. for long job descriptions\n",
        "    # for use with Pandas .apply() function\n",
        "\n",
        "    text = str(text) #convert to string\n",
        "\n",
        "    if isinstance(text, list):\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    if len(text) <= num:\n",
        "        return text\n",
        "    else:\n",
        "        short_text = text[:num]\n",
        "        return short_text + \"..\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKpVSo1HFMV-"
      },
      "source": [
        "### optimize k-means no.\n",
        "\n",
        "stole this snippet from a different project of mine - was written several months ago, don't judge the efficiency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FdgV9BNAF5n"
      },
      "source": [
        "from kneed import KneeLocator\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def optimal_num_clustas(input_matrix, d_title, top_end=11, show_plot=False,\n",
        "                        write_image=False):\n",
        "    # given 'input_matrix' as a pandas series containing a list / vector in each\n",
        "    # row, find the optimal number of k_means clusters to cluster them using \n",
        "    # the elbow method\n",
        "\n",
        "    # 'top_end' is the max number of clusters. If having issues, look at the plot\n",
        "    # and adjust accordingly\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    # texthero input data structure is weird.\n",
        "    #  stole the below if/else from the source code behind TH kmeans fn\n",
        "    # https://github.com/jbesomi/texthero/blob/master/texthero/representation.py\n",
        "\n",
        "    if isinstance(input_matrix, pd.DataFrame):\n",
        "        # fixes weird issues parsing a texthero edited text pd series\n",
        "        input_matrix_coo = input_matrix.sparse.to_coo()\n",
        "        input_matrix_for_vectorization = input_matrix_coo.astype(\"float64\")\n",
        "    else:\n",
        "        input_matrix_for_vectorization = list(input_matrix)\n",
        "\n",
        "    scaled_features = scaler.fit_transform(input_matrix_for_vectorization)\n",
        "    kmeans_kwargs = {\n",
        "        \"init\": \"random\",\n",
        "        \"n_init\": 30,\n",
        "        \"max_iter\": 300,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    # A list holds the SSE values for each k\n",
        "    sse = []\n",
        "    for k in range(1, top_end):\n",
        "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
        "        kmeans.fit(scaled_features)\n",
        "        sse.append(kmeans.inertia_)\n",
        "\n",
        "    # plot to illustrate (viewing it is optional)\n",
        "    title_k = 'Optimal k-means for' + d_title\n",
        "    kmeans_opt_df = pd.DataFrame(list(zip(range(1, top_end), sse)), columns=['Number of Clusters', 'SSE'])\n",
        "    f_k = px.line(kmeans_opt_df, x='Number of Clusters', y='SSE', title=title_k)\n",
        "    # find optimum\n",
        "    kl = KneeLocator(range(1, top_end), sse, \n",
        "                     curve=\"convex\", direction=\"decreasing\")\n",
        "    onk = kl.elbow\n",
        "\n",
        "    if onk is None:\n",
        "        print(\"Warning - {} has no solution for optimal k-means\".format(d_title))\n",
        "        print(\"Returning # of clusters as max allowed ( {} )\".format(top_end))\n",
        "        return top_end\n",
        "\n",
        "    if onk == top_end:\n",
        "        print(\"Warning - {} opt. # equals max value searched ({})\".format(d_title,\n",
        "                                                                          top_end))\n",
        "\n",
        "    print(\"\\nFor {}: opt. # of k-means clusters is {} \\n\".format(d_title, onk))\n",
        "    f_k.add_vline(x=onk) # add vertical line to plotly \n",
        "\n",
        "    if show_plot:\n",
        "        f_k.show() \n",
        "\n",
        "    if write_image:\n",
        "        f_k.write_image(join(output_path_full, title_k + \".png\"))\n",
        "\n",
        "    return onk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNYJN5pwsP9z"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBiXnF2jzou_"
      },
      "source": [
        "## visualization \n",
        "\n",
        "Use texthero built-in visualization methods and then plot them *explicitly* in Plotly because it has dark mode and TextHero's plotly wrapper function does not\n",
        "\n",
        "- texthero represendation [docs](https://texthero.org/docs/api-representation)\n",
        "\n",
        "- plotly [scatterplot overview](https://plotly.com/python/line-and-scatter/)\n",
        "    - px.scatter() detailed [documentation](https://plotly.com/python-api-reference/generated/plotly.express.scatter)\n",
        "\n",
        "### Options:\n",
        "1. ```viz_job_data(df)``` is the OG version, vectorizes text with TFIDF and plots with PCA. \n",
        "    - Uses k-means nclusters as 5\n",
        "2. ```viz_job_data_word2vec(df)``` vectorizes text with word2vec (*computing \"average\" 300-d vector*) and plots with PCA\n",
        "    - computes optimal # of kmeans clusters (max set to 15)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyYd-hdx7YWb"
      },
      "source": [
        "### standard\n",
        "\n",
        "\n",
        "uses everything built-in to the texthero package:\n",
        "- keeps kmeans clusters @ 5\n",
        "- text is converted to vector based on tf-idf\n",
        "    - (anecdotally this doesn't seem to work well if your \"document\" is short like these job listings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlpav-ahzo07"
      },
      "source": [
        "from datetime import datetime, date\n",
        "import plotly.express as px\n",
        "\n",
        "plots_made = 0\n",
        "\n",
        "def viz_job_data(viz_df, text_col_name, save_plot=False, h=720):\n",
        "\n",
        "    today = date.today()\n",
        "    # Month abbreviation, day and year\t\n",
        "    td_str = today.strftime(\"%b-%d-%Y\")\n",
        "\n",
        "    viz_df['tfidf'] = (\n",
        "        viz_df[text_col_name]\n",
        "        .pipe(hero.clean)\n",
        "        .pipe(hero.tfidf)\n",
        "    )\n",
        "\n",
        "    viz_df['kmeans'] = (\n",
        "        viz_df['tfidf']\n",
        "        .pipe(hero.kmeans, n_clusters=5)\n",
        "        .astype(str)\n",
        "    )\n",
        "\n",
        "    viz_df['pca'] = (\n",
        "        viz_df['tfidf']\n",
        "        .pipe(hero.pca)\n",
        "    )\n",
        "\n",
        "    hv_list = list(viz_df.columns)\n",
        "    hv_list.remove('tfidf')\n",
        "    hv_list.remove('pca')\n",
        "    hv_list.remove('summary')\n",
        "\n",
        "    plot_title = td_str + \" Vizualize Companies by {} Data\".format(text_col_name)\n",
        "\n",
        "    # reformat data so don't have to use built-in plotting \n",
        "\n",
        "    df_split_pca = pd.DataFrame(viz_df[\"pca\"].to_list(), columns=['pca_x', 'pca_y'])\n",
        "    viz_df.drop(columns=\"pca\", inplace=True) # drop original PCA column\n",
        "    viz_df = pd.concat([viz_df, df_split_pca], axis=1) # merge dataframes\n",
        "\n",
        "    # plot pca data\n",
        "    # texthero also features pther ways to reduce dimensions besides pca, see docs\n",
        "\n",
        "    w = int(h * (4/3))\n",
        "\n",
        "    fig_s = px.scatter(viz_df, x= \"pca_x\", y=\"pca_y\", color=\"kmeans\", \n",
        "                     hover_data=hv_list, title=plot_title, height=h, width=w,\n",
        "                     template=\"plotly_dark\")\n",
        "    fig_s.show()\n",
        "\n",
        "    if save_plot:\n",
        "        plots_made += 1\n",
        "\n",
        "        fig.write_html(plot_title + str(plots_made) + \".html\", \n",
        "                       include_plotlyjs=True)\n",
        "\n",
        "    print(\"plot generated - \", datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywbr89TOwiRp"
      },
      "source": [
        "### word2vec viz\n",
        "\n",
        "- word2vec [explanation](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) for those uninitiated\n",
        "-gensim [link](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html)\n",
        "\n",
        "Re: using word2vec in a basic way for sentence embedding\n",
        "\n",
        "> With the Word2Vec model, we can calculate the vectors for each word in a document. But what if we want to calculate a vector for the entire document? We could average the vectors for each word in the document - while this is quick and crude, it can often be useful. However, there is a better wayâ€¦ \n",
        "> *goes on to discuss doc2vec which does not have pretrained models*\n",
        "\n",
        "---\n",
        "\n",
        "re: ^ I am not doing the better way. Load the word2vec-google-news-300 dataset from gensim, then iterate through all words in the input text, generate the vector for that word, then take the mean of those. Only words with length 3 or greater are considered. \n",
        "\n",
        "---\n",
        "<font color='orange'>**DISCLAIMER: IT TAKES A BIT TO DOWNLOAD THE GENSIM / GOOGLE MODEL**</font>\n",
        "- <font color='orange'>possible to save the model locally (on disk/gdrive) if desired</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk4Pi_PFwhkL"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# model = api.load(\"word2vec-ruscorpora-300\")\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "print(\"loaded data for word2vec - \", datetime.now())\n",
        "\n",
        "print(\"test the model\")\n",
        "test_string = \"computer\"\n",
        "vector = model.wv[test_string]\n",
        "\n",
        "print(\"The shape of string {} is: \\n {}\".format(test_string, vector.shape))\n",
        "pp.pprint(vector.shape, compact=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Ji5Lg9ELit"
      },
      "source": [
        "iterate through all words in the input text generate the word2vec vector for that word, then take the mean of those. Only words with length 3 or greater are considered.\n",
        "\n",
        "**note that if a word is not in the google news dataset, it is just skipped. If you think the graphs / representations are not good enough, you should check to ensure that a large amount of words (or fraction rather) are not being skipped)**\n",
        "* the verbose parameter helps with that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn7CHPJZwhAv"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_vector_freetext(input_text, verbose=0, cutoff=2):\n",
        "    # always uses locally initialized model, i.e. \"model\" variable (see above cells)\n",
        "\n",
        "    # verbose = 1 shows you how many words were skipped\n",
        "    # verbose = 2 tells you each individual skipped word and ^\n",
        "    # 'cutoff' removes all words with length N or less from the rep. vector\n",
        "\n",
        "    lower_it = input_text.lower()\n",
        "    input_words = lower_it.split(\" \") # yes, this is an assumption\n",
        "    usable_words = [word for word in input_words if len(word) > cutoff]\n",
        "\n",
        "    list_of_vectors = []\n",
        "    num_words_total = len(usable_words)\n",
        "    num_excluded = 0\n",
        "\n",
        "    for word in usable_words:\n",
        "        try:\n",
        "            this_vector = model.wv[word]\n",
        "            list_of_vectors.append(this_vector)\n",
        "        except:\n",
        "            num_excluded += 1\n",
        "            if verbose == 2:\n",
        "                print(\"\\nThe word/term {} is not in the model vocab.\".format(word))\n",
        "                print(\"Excluding from representative vector\")\n",
        "\n",
        "    rep_vec = np.mean(list_of_vectors, axis=0)\n",
        "\n",
        "    if verbose > 0:\n",
        "        print(\"Computed representative vector. Excluded {} words out of {}\".format(num_excluded,\n",
        "                                                                            num_words_total))\n",
        "\n",
        "    return rep_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8NdBrCiyAnt"
      },
      "source": [
        "#### Plotting function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VinUUQ7uwFqZ"
      },
      "source": [
        "from datetime import datetime, date\n",
        "import plotly.express as px\n",
        "\n",
        "def viz_job_data_word2vec(viz_df, text_col_name, save_plot=False, h=720,\n",
        "                          query_name=\"\", show_text=False):\n",
        "\n",
        "    today = date.today()\n",
        "    # Month abbreviation, day and year\t\n",
        "    td_str = today.strftime(\"%b-%d-%Y\")\n",
        "\n",
        "    # compute word2vec avg vector for each row of text\n",
        "    viz_df['avg_vec'] = viz_df[text_col_name].apply(get_vector_freetext)\n",
        "\n",
        "    # get optimal number of kmeans. limit max to 15 for interpretability\n",
        "    max_clusters = 15\n",
        "    if len(viz_df['avg_vec']) < max_clusters: max_clusters = len(viz_df['avg_vec'])\n",
        "\n",
        "    kmeans_numC = optimal_num_clustas(viz_df['avg_vec'], \n",
        "                                      d_title='word2vec-' + query_name, \n",
        "                                      top_end=max_clusters)\n",
        "    \n",
        "    # complete k-means clustering + pca dim red. w/ avg_vec\n",
        "    if kmeans_numC is None:\n",
        "        kmeans_numC = 5 \n",
        "\n",
        "    viz_df['kmeans'] = (\n",
        "        viz_df['avg_vec']\n",
        "        .pipe(hero.kmeans, n_clusters=kmeans_numC, \n",
        "              algorithm=\"elkan\", random_state=42, n_init=30)\n",
        "        .astype(str)\n",
        "    )\n",
        "    # texthero has other algs to reduce dimensions besides pca, see docs\n",
        "    viz_df['pca'] = (\n",
        "        viz_df['avg_vec']\n",
        "        .pipe(hero.pca)\n",
        "    )\n",
        "\n",
        "    # generate list of column names for hover_data\n",
        "    hv_list = list(viz_df.columns)\n",
        "    hv_list.remove('avg_vec')\n",
        "    hv_list.remove('pca')\n",
        "    if \"tfidf\" in hv_list:\n",
        "        hv_list.remove('tfidf')\n",
        "    if \"summary\" in hv_list:\n",
        "        hv_list.remove('summary')\n",
        "\n",
        "    # reformat data so don't have to use texthero built-in plotting \n",
        "    df_split_pca = pd.DataFrame(viz_df[\"pca\"].to_list(), \n",
        "                                columns=['pca_x', 'pca_y'])\n",
        "    viz_df.drop(columns=\"pca\", inplace=True) # drop original PCA column\n",
        "    viz_df = pd.concat([viz_df, df_split_pca], axis=1) # merge dataframes\n",
        "\n",
        "    # set up plot pars (width, title, text)\n",
        "    w = int(h * (4/3))\n",
        "\n",
        "    if len(query_name) > 0:\n",
        "        # user provided query_name so include\n",
        "        plot_title = td_str + \" Vizualize Companies by {} text w word2vec\".format(text_col_name) + \" | \" + query_name\n",
        "    else:\n",
        "        plot_title = td_str + \" Vizualize Companies by {} text w word2vec\".format(text_col_name)\n",
        "\n",
        "    if show_text:\n",
        "        # adds company names to the plot if you want\n",
        "        viz_df[\"companies_abbrev\"] = viz_df[\"companies\"].apply(text_first_N, num=15)\n",
        "        graph_text_label = \"companies_abbrev\"\n",
        "    else:\n",
        "        graph_text_label = None\n",
        "    \n",
        "    # plot dimension-reduced data\n",
        "    fig_w2v = px.scatter(viz_df, x= \"pca_x\", y=\"pca_y\", color=\"kmeans\", \n",
        "                     hover_data=hv_list, title=plot_title, height=h, width=w,\n",
        "                     template=\"plotly_dark\", text=graph_text_label)\n",
        "    fig_w2v.show()\n",
        "\n",
        "    # save if requested \n",
        "    \n",
        "    if save_plot:\n",
        "        # saves the HTML file \n",
        "        # auto-saving as a static image is a lil difficult so just click on the interactive\n",
        "        # plot it generates \n",
        "        fig_w2v.write_html(plot_title + query_name + \"_\" + text_col_name + \".html\", \n",
        "                       include_plotlyjs=True)\n",
        "\n",
        "    print(\"plot generated - \", datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXL23kllsMn8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC593SxF473a"
      },
      "source": [
        "## CH get_jobs() general fn\n",
        "\n",
        "The main function find_jobs_from() adjusted to work for CH\n",
        "\n",
        "\n",
        "<font color='orange'>if customizing this code, you need to change this</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ujVfjl91mVh"
      },
      "source": [
        "from datetime import date\n",
        "import pprint as pp\n",
        "\n",
        "today = date.today()\n",
        "# Month abbreviation, day and year\t\n",
        "d4 = today.strftime(\"%b-%d-%Y\")\n",
        "\n",
        "default_filename = d4 + \"_[raw]_scraped_jobs_CH.xls\"\n",
        "\n",
        "def find_CHjobs_from(website, desired_characs, job_query, job_type=None, \n",
        "                   language=None, filename=default_filename, verbose=False):    \n",
        "    \"\"\"\n",
        "    This function extracts all the desired characteristics of all new job postings\n",
        "    of the title and location specified and returns them in single file.\n",
        "    The arguments it takes are:\n",
        "        - Website: to specify which website to search (options: 'indeed' or 'indeed_default')\n",
        "        - job_query: words that you want to narrow down the jobs to. for example 'data'\n",
        "        - job_type: 'internship' or 'fulltime' or 'permanent'\n",
        "        - language: 'en' or 'de' or leave blank\n",
        "        - Desired_characs: this is a list of the job characteristics of interest,\n",
        "            from titles, companies, links and date_listed.\n",
        "        - Filename: default is \"JS_test_results.xls\", can be changed to whatever \n",
        "    \"\"\"\n",
        "    \n",
        "    if website == 'indeed':\n",
        "        sp_search = load_indeed_jobs_CH(job_query, \n",
        "                                       job_type=job_type, language=language)\n",
        "        job_soup = sp_search.get(\"job_soup\")\n",
        "        URL_used = sp_search.get(\"query_URL\")\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n The full HTML docs are: \\n\")\n",
        "            pp.pprint(job_soup, compact=True)\n",
        "        jobs_list, num_listings = extract_job_information_indeedCH(job_soup, \n",
        "                                                                   desired_characs,\n",
        "                                                                   uURL=URL_used)\n",
        "    elif website == 'indeed_default':\n",
        "        sp_search = load_indeed_jobs_CH(job_query, run_default=True)\n",
        "        job_soup = sp_search.get(\"job_soup\")\n",
        "        URL_used = sp_search.get(\"query_URL\")\n",
        "        if verbose:\n",
        "            print(\"\\n The full HTML docs are: \\n\")\n",
        "            pp.pprint(job_soup, compact=True)\n",
        "\n",
        "        jobs_list, num_listings = extract_job_information_indeedCH(job_soup, \n",
        "                                                                   desired_characs,\n",
        "                                                                   uURL=URL_used)\n",
        "\n",
        "    job_df = save_jobs_to_excel(jobs_list, filename)\n",
        " \n",
        "    print('{} new job postings retrieved from {}. Stored in {}.'.format(num_listings, \n",
        "                                                                          website, filename))\n",
        "    \n",
        "    return job_df\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLwW1mEe2P9F"
      },
      "source": [
        "## Indeed-scraping\n",
        "\n",
        "<font color='orange'>if customizing this code, you *may* need to change these </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WViuK5h3eDFc"
      },
      "source": [
        "### main job scraping fn\n",
        "\n",
        "\n",
        "links used for testing / figuring out URL structure\n",
        "\n",
        "```\n",
        "website = \"https://ch.indeed.com/Switzerland-English-Jobs\"\n",
        "base ->\n",
        "example search https://ch.indeed.com/Stellen?q=Switzerland+English&jt=internship\n",
        "data job, language = EN, no others: https://ch.indeed.com/Stellen?q=data&lang=en\n",
        "^ but internship https://ch.indeed.com/Stellen?q=data&jt=internship&lang=en\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNpH9i8awJNX"
      },
      "source": [
        "## added by Peter\n",
        "\n",
        "## indeed switzerland -------------------------------------------------------\n",
        "\n",
        "def load_indeed_jobs_CH(job_query, job_type=None, language=None, \n",
        "                        run_default=False):\n",
        "    i_website = \"https://ch.indeed.com/Stellen?\"\n",
        "    def_website = \"https://ch.indeed.com/Stellen?q=Switzerland+English&jt=internship\"\n",
        "    if run_default:\n",
        "        # switzerland has a unique page shown below, can run by default\n",
        "        # website = \"https://ch.indeed.com/Switzerland-English-Jobs\"\n",
        "\n",
        "        getVars = {'fromage' : 'last', \"limit\": '50', 'sort' : 'date'}\n",
        "\n",
        "        url = (def_website + urllib.parse.urlencode(getVars))\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "        job_soup = soup.find(id=\"resultsCol\")\n",
        "    else:\n",
        "        getVars = {'q' : job_query, 'jt' : job_type, 'lang' : language,\n",
        "                'fromage' : 'last', \"limit\": '50', 'sort' : 'date'}\n",
        "\n",
        "        # if values are not specified, then remove them from the dict (and URL)\n",
        "        if job_query is None:\n",
        "            del getVars['q']\n",
        "        if job_type is None:\n",
        "            del getVars['jt']\n",
        "        if language is None:\n",
        "            del getVars['lang']\n",
        "\n",
        "        url = (i_website + urllib.parse.urlencode(getVars))\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "        job_soup = soup.find(id=\"resultsCol\")\n",
        "\n",
        "    # return the job soup\n",
        "\n",
        "    soup_results = {\n",
        "        \"job_soup\": job_soup,\n",
        "        \"query_URL\": url\n",
        "\n",
        "    }\n",
        "    return soup_results\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JtsmHpceIly"
      },
      "source": [
        "### helper / parsing functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCZ31s1R2RJ-"
      },
      "source": [
        "import pprint as pp\n",
        "\n",
        "def_URL = \"https://ch.indeed.com/Stellen?\" + \"ADD_queries_here\" \n",
        "\n",
        "def extract_job_information_indeedCH(job_soup, desired_characs, uURL=def_URL, \n",
        "                                     verbose=False):\n",
        "    job_elems = job_soup.find_all('div', class_='jobsearch-SerpJobCard')\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nAll found 'job elements' are as follows: \\n\")\n",
        "        pp.pprint(job_elems, compact=True)\n",
        "    \n",
        "    with open('job_elements.txt', 'w') as f:\n",
        "        # save to text file for investigation\n",
        "        print(job_elems, file=f)\n",
        "    \n",
        "    cols = []\n",
        "    extracted_info = []\n",
        "    \n",
        "    \n",
        "    if 'titles' in desired_characs:\n",
        "        titles = []\n",
        "        cols.append('titles')\n",
        "        for job_elem in job_elems:\n",
        "            titles.append(extract_job_title_indeed(job_elem))\n",
        "        extracted_info.append(titles)                    \n",
        "    \n",
        "    if 'companies' in desired_characs:\n",
        "        companies = []\n",
        "        cols.append('companies')\n",
        "        for job_elem in job_elems:\n",
        "            companies.append(extract_company_indeed(job_elem))\n",
        "        extracted_info.append(companies)\n",
        "    \n",
        "    if 'date_listed' in desired_characs:\n",
        "        dates = []\n",
        "        cols.append('date_listed')\n",
        "        for job_elem in job_elems:\n",
        "            dates.append(extract_date_indeed(job_elem))\n",
        "        extracted_info.append(dates)\n",
        "    \n",
        "    if 'summary' in desired_characs:\n",
        "        summaries = []\n",
        "        cols.append('summary')\n",
        "        for job_elem in job_elems:\n",
        "            summaries.append(extract_summary_indeed(job_elem))\n",
        "        extracted_info.append(summaries)\n",
        "\n",
        "    if 'links' in desired_characs:\n",
        "        links = []\n",
        "        cols.append('links')\n",
        "        for job_elem in job_elems:\n",
        "            links.append(extract_link_indeedCH(job_elem, uURL))\n",
        "        extracted_info.append(links)\n",
        "    \n",
        "    jobs_list = {}\n",
        "    \n",
        "    for j in range(len(cols)):\n",
        "        jobs_list[cols[j]] = extracted_info[j]\n",
        "    \n",
        "    num_listings = len(extracted_info[0])\n",
        "    \n",
        "    return jobs_list, num_listings\n",
        "\n",
        "\n",
        "def extract_job_title_indeed(job_elem):\n",
        "    title_elem = job_elem.find('h2', class_='title')\n",
        "    title = title_elem.text.strip()\n",
        "    return title\n",
        "\n",
        "def extract_company_indeed(job_elem):\n",
        "    company_elem = job_elem.find('span', class_='company')\n",
        "    company = company_elem.text.strip()\n",
        "    return company\n",
        "\n",
        "def extract_link_indeedCH(job_elem, uURL):\n",
        "    # some manual shenanigans occur here\n",
        "    # working example https://ch.indeed.com/Stellen?q=data&jt=internship&lang=en&vjk=49ed864bd5e422fb\n",
        "\n",
        "    link = job_elem.find('a')['href']\n",
        "    uURL_list = uURL.split(\"&fromage=last\")\n",
        "    link = uURL_list[0] + \"&\" + link\n",
        "    # replace some text so that the link has a virtual job key. Found via trial and error\n",
        "    return link.replace(\"/rc/clk?jk=\", \"vjk=\")\n",
        "\n",
        "def extract_date_indeed(job_elem):\n",
        "    date_elem = job_elem.find('span', class_='date')\n",
        "    date = date_elem.text.strip()\n",
        "    return date\n",
        "\n",
        "def extract_summary_indeed(job_elem):\n",
        "    summary_elem = job_elem.find('div', class_='summary')\n",
        "    summary = summary_elem.text.strip()\n",
        "    return summary\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x8_xHCyeMVy"
      },
      "source": [
        "### postprocess\n",
        "\n",
        "1. input dataframe of scraped jobs from indeed\n",
        "2. ???\n",
        "3. output sexy_df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKrodaoTeaRU"
      },
      "source": [
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "def indeed_postprocess(i_df, query_term, query_jobtype, verbose=False,\n",
        "                       shorten_links=False, download_excel=False):\n",
        "    print(\"Starting postprocess - \", datetime.now())\n",
        "\n",
        "    # apply texthero cleaning\n",
        "    i_df[\"titles\"] = hero.clean(i_df[\"titles\"])\n",
        "    i_df[\"summary\"] = hero.clean(i_df[\"summary\"])\n",
        "\n",
        "    # use bit.ly to shorten links\n",
        "    if shorten_links:\n",
        "        try:\n",
        "            len(i_df[\"short_link\"])\n",
        "            print(\"found values for short_link, not-recreating\")\n",
        "        except:\n",
        "            print(\"no values exist for short_link, creating them now\")\n",
        "            # there is a random delay to not overload APIs, max rt is 5s * num_rows\n",
        "            i_df[\"short_link\"] = i_df[\"links\"].apply(shorten_URL_bitly) \n",
        "    else:\n",
        "        i_df[\"short_link\"] = \"not_created\"\n",
        "\n",
        "    # save file to excel\n",
        "    rn = datetime.now()\n",
        "    i_PP_date = rn.strftime(\"_%m.%d.%Y-%H-%M_\")\n",
        "    i_df[\"date_pulled\"] = rn.strftime(\"%m.%d.%Y\")\n",
        "    i_df[\"time_pulled\"] = rn.strftime(\"%H:%M:%S\")\n",
        "    out_name = \"JS_DB_\" + \"query=[term(s)=\" + query_term + \", type=\" + query_jobtype + \"]\" + i_PP_date + \".xlsx\"\n",
        "    i_df.to_excel(out_name)\n",
        "    if verbose: print(\"Saved {} - \".format(out_name), datetime.now())\n",
        "\n",
        "    # download if requested\n",
        "    if download_excel:\n",
        "        files.download(out_name)\n",
        "        if verbose: print(\"Downloaded {} - \".format(out_name), datetime.now())\n",
        "\n",
        "\n",
        "    return i_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEfJ6a8Plg4l"
      },
      "source": [
        "quick printout of the datatable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CzjUiTkikDa"
      },
      "source": [
        "def indeed_datatable(i_df, count_what=\"companies\", freq_n=10):\n",
        "    # basically just wrote this to reduce code down below\n",
        "    # depends on the colab data_table.DataTable()\n",
        "    \n",
        "    print(\"Count of column '{}' appearances in search:\\n\".format(count_what))\n",
        "    comp_list_1 = i_df[count_what].value_counts()\n",
        "    pp.pprint(comp_list_1.head(freq_n), compact=True)  \n",
        "\n",
        "    i_df_disp = i_df.copy()\n",
        "    i_df_disp[\"summary_short\"] = i_df_disp[\"summary\"].apply(text_first_N)\n",
        "    i_df_disp.drop(columns=[\"links\", \"summary\"], inplace=True) # drop verbose columns\n",
        "\n",
        "    return i_df_disp \n",
        "\n",
        "    # i_df_disp for use inside \n",
        "    # data_table.DataTable(i_df_disp, \n",
        "                        #  include_index=False, num_rows_per_page=table_n)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhZ8o2jrsTIQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV_apCm-iUXu"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHCmgk5Hm4yj"
      },
      "source": [
        "## parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWGKOn2qcXhY"
      },
      "source": [
        "# determines columns in output dataframe post-scraping\n",
        "desired_characs = ['titles', 'companies', 'links', 'date_listed', 'summary']\n",
        "\n",
        "# TODO - add other parameters used in functions below here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IMX9kI95Awh"
      },
      "source": [
        "## Swiss Jobs - Indeed\n",
        "\n",
        "```\n",
        "This function extracts all the desired characteristics of all new job postings\n",
        "    of the title and location specified and returns them in single file.\n",
        "The arguments it takes are:\n",
        "\n",
        "    - Website: to specify which website to search \n",
        "        - (options: 'indeed' or 'indeed_default')\n",
        "    - job_query: words that you want to narrow down the jobs to. \n",
        "        - for example 'data'\n",
        "    - job_type: \n",
        "        - 'internship' or 'fulltime' or 'permanent'\n",
        "    - language: \n",
        "        - 'en' or 'de' or other languages.. 'fr'? ew\n",
        "    - Desired_characs: what columns of data do you want to extract? options are:\n",
        "        - 'titles', 'companies', 'links', 'date_listed', 'summary'\n",
        "    - Filename: default is \"JS_test_results.xls\", can be changed to whatever \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9RngcapfT9D"
      },
      "source": [
        "### query 1 - internship in \"data\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLme2DE05A8J"
      },
      "source": [
        "\n",
        "from google.colab import data_table\n",
        "\n",
        "jq1=\"data\"\n",
        "jt1 = \"internship\"\n",
        "lan = \"en\"\n",
        "\n",
        "chdf1 = find_CHjobs_from(website=\"indeed\", desired_characs=desired_characs, \n",
        "                         job_query=jq1, job_type=jt1, language=lan)\n",
        "\n",
        "q1_processed = indeed_postprocess(chdf1, query_term=jq1, query_jobtype=jt1,\n",
        "                       shorten_links=True, download_excel=True)\n",
        "\n",
        "data_table.DataTable(indeed_datatable(q1_processed), \n",
        "                     include_index=False, num_rows_per_page=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oly6RmkVvKkv"
      },
      "source": [
        "**Viz Query 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8g-Vivnvaso"
      },
      "source": [
        "viz1 = q1_processed.copy()\n",
        "viz1.drop(columns=[\"links\", \"short_link\"], inplace=True)\n",
        "\n",
        "viz_job_data_word2vec(viz1, \"summary\", save_plot=True, show_text=True,\n",
        "                      query_name=jt1 + \" in \" + jq1)\n",
        "viz_job_data_word2vec(viz1, \"titles\", save_plot=True, show_text=False,\n",
        "                      query_name=\"internships in -data-\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl1s68pygQNa"
      },
      "source": [
        "### query 2 - all jobs in Switzerland for English Speakers\n",
        "\n",
        "*This is the special case where Indeed shows you all CH-English jobs and it doesn't follow standard search query logic / HTML (has its own page)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9abIK_KOgVqW"
      },
      "source": [
        "jq2 = \"indeed_default\"\n",
        "jt2 = \"all\"\n",
        "# in the case of \"run the special case on Indeed\" query terms don't matter\n",
        "chdf2 = find_CHjobs_from(website=\"indeed_default\", job_query=\"gimme\", \n",
        "                         desired_characs=desired_characs)\n",
        "    \n",
        "jq2 = \"indeed_default\"\n",
        "jt2 = \"all\"\n",
        "\n",
        "q2_processed = indeed_postprocess(chdf2, query_term=jq2, query_jobtype=jt2,\n",
        "                       shorten_links=False, download_excel=False)\n",
        "\n",
        "data_table.DataTable(indeed_datatable(q2_processed), \n",
        "                     include_index=False, num_rows_per_page=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SOoPK-9uTcY"
      },
      "source": [
        "**Viz Query 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SB27BEB0yux"
      },
      "source": [
        "viz_q2 = q2_processed.copy()\n",
        "viz_q2.drop(columns=\"links\", inplace=True)\n",
        "viz_job_data_word2vec(viz_q2, \"summary\", save_plot=False, show_text=True,\n",
        "                      query_name=\"all lan='en' jobs in CH\")\n",
        "viz_job_data_word2vec(viz_q2, \"titles\", save_plot=False, show_text=False,\n",
        "                      query_name=\"all lan='en' jobs in CH\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}